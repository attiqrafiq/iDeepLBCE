{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python version 3.10.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# If running locally or on a fresh runtime, uncomment the next line:\n",
    "!pip install -q transformers>=4.40 accelerate torch scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T20:42:03.646902Z",
     "iopub.status.busy": "2025-09-02T20:42:03.646289Z",
     "iopub.status.idle": "2025-09-02T20:42:03.741607Z",
     "shell.execute_reply": "2025-09-02T20:42:03.740861Z",
     "shell.execute_reply.started": "2025-09-02T20:42:03.646876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"wand_ai_api_uog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T20:42:04.816600Z",
     "iopub.status.busy": "2025-09-02T20:42:04.815920Z",
     "iopub.status.idle": "2025-09-02T20:42:12.999131Z",
     "shell.execute_reply": "2025-09-02T20:42:12.998581Z",
     "shell.execute_reply.started": "2025-09-02T20:42:04.816575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T21:10:58.400106Z",
     "iopub.status.busy": "2025-09-02T21:10:58.399351Z",
     "iopub.status.idle": "2025-09-02T21:10:58.440720Z",
     "shell.execute_reply": "2025-09-02T21:10:58.439977Z",
     "shell.execute_reply.started": "2025-09-02T21:10:58.400073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    matthews_corrcoef, roc_auc_score, average_precision_score\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    AutoModel, Trainer, TrainingArguments, DataCollatorWithPadding, set_seed\n",
    ")\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "# -------------------- #\n",
    "# FASTA utilities\n",
    "# -------------------- #\n",
    "def read_fasta(path):\n",
    "    recs = []\n",
    "    header, seq_lines = None, []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            if line.startswith(\">\"):\n",
    "                if header: recs.append((header, \"\".join(seq_lines)))\n",
    "                header, seq_lines = line[1:], []\n",
    "            else:\n",
    "                seq_lines.append(line)\n",
    "    if header: recs.append((header, \"\".join(seq_lines)))\n",
    "    return recs\n",
    "\n",
    "def sanitize_seq(seq: str) -> str:\n",
    "    allowed = set(\"ACDEFGHIKLMNPQRSTVWYBXZJUO\")\n",
    "    seq = seq.upper().replace(\"*\", \"\")\n",
    "    return \"\".join([ch if ch in allowed else \"X\" for ch in seq])\n",
    "\n",
    "def infer_label(h: str) -> Optional[int]:\n",
    "    hl = h.lower()\n",
    "    if \"negative\" in hl or \"non-epitope\" in hl: return 0\n",
    "    if \"positive\" in hl or \"epitope\" in hl: return 1\n",
    "    if \"neg\" in hl and \"pos\" not in hl: return 0\n",
    "    if \"pos\" in hl and \"neg\" not in hl: return 1\n",
    "    m = re.search(r\"(label|class)[:=]\\s*([01])\", hl)\n",
    "    if m: return int(m.group(2))\n",
    "    return None\n",
    "\n",
    "def load_dataset_from_fasta(fasta_path: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for h, s in read_fasta(fasta_path):\n",
    "        lab = infer_label(h)\n",
    "        if lab is None:\n",
    "            raise ValueError(f\"Could not infer label from header: {h}\")\n",
    "        rows.append({\"header\": h, \"sequence\": sanitize_seq(s), \"label\": lab})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _maybe_infer_labels_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If headers contain obvious label hints, keep them; otherwise set label = NaN.\n",
    "    \"\"\"\n",
    "    if \"label\" not in df.columns:\n",
    "        df[\"label\"] = np.nan\n",
    "    # Keep whatever load_dataset_from_fasta produced; it already tries to infer.\n",
    "    # If that raised earlier for missing labels, you can add a separate loader for unlabeled FASTA.\n",
    "    return df\n",
    "\n",
    "def _effective_max_len(self) -> int:\n",
    "    \"\"\"\n",
    "    ESM2 models typically cap around 1022 tokens incl. specials. Respect tokenizer.model_max_length if set.\n",
    "    \"\"\"\n",
    "    # transformers may give a giant int for \"very long\". Clamp to 1022 as a safe ceiling for ESM2.\n",
    "    tk_max = getattr(self.tokenizer, \"model_max_length\", 1024)\n",
    "    safe_cap = 1022\n",
    "    return int(min(self.max_length, tk_max if tk_max < 10_000 else safe_cap))\n",
    "    \n",
    "# -------------------- #\n",
    "# Dataset wrapper\n",
    "# -------------------- #\n",
    "class ESM2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=1022):\n",
    "        self.df, self.tokenizer, self.max_length = df, tokenizer, max_length\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        enc = self.tokenizer(\n",
    "            row[\"sequence\"], truncation=True, max_length=self.max_length,\n",
    "            return_tensors=\"pt\", add_special_tokens=True\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "# -------------------- #\n",
    "# Main Pipeline Class\n",
    "# -------------------- #\n",
    "class ESM2Pipeline:\n",
    "    def __init__(self,\n",
    "                 model_name=\"facebook/esm2_t12_35M_UR50D\",\n",
    "                 outdir=\"./esm2_out\",\n",
    "                 max_length=1022,\n",
    "                 batch_size=8,\n",
    "                 lr=2e-5,\n",
    "                 epochs=5,\n",
    "                 weight_decay=0.01,\n",
    "                 seed=42,\n",
    "                 fp16=True,\n",
    "                 BF16=True,\n",
    "                 freeze_encoder=False,\n",
    "                 class_weighted_loss=True):\n",
    "        self.model_name = model_name\n",
    "        self.outdir = outdir\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.weight_decay = weight_decay\n",
    "        self.seed = seed\n",
    "        self.fp16 = fp16\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.class_weighted_loss = class_weighted_loss\n",
    "\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        set_seed(seed)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        def _compute(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            if isinstance(logits, (tuple, list)): logits = logits[0]\n",
    "            probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "            preds = probs.argmax(axis=1)\n",
    "            y_true = labels\n",
    "            y_proba = probs[:, 1]\n",
    "            return {\n",
    "                 \"precision\": precision_score(y_true, preds, zero_division=0),\n",
    "               \"accuracy\": accuracy_score(y_true, preds),\n",
    "                \"recall\": recall_score(y_true, preds, zero_division=0),\n",
    "                \"f1\": f1_score(y_true, preds, zero_division=0),\n",
    "                \"mcc\": matthews_corrcoef(y_true, preds),\n",
    "                \"roc_auc\": roc_auc_score(y_true, y_proba),\n",
    "                \"pr_auc\": average_precision_score(y_true, y_proba),\n",
    "            }\n",
    "        return _compute\n",
    "\n",
    "    def train_and_eval(self, train_fasta, ind_fasta, val_size=0.1):\n",
    "        df_train_all = load_dataset_from_fasta(train_fasta)\n",
    "        df_ind = load_dataset_from_fasta(ind_fasta)\n",
    "\n",
    "        df_tr, df_val = train_test_split(df_train_all, test_size=val_size,\n",
    "                                         random_state=self.seed, stratify=df_train_all[\"label\"])\n",
    "\n",
    "        train_ds = ESM2Dataset(df_tr, self.tokenizer, self.max_length)\n",
    "        val_ds = ESM2Dataset(df_val, self.tokenizer, self.max_length)\n",
    "        ind_ds = ESM2Dataset(df_ind, self.tokenizer, self.max_length)\n",
    "\n",
    "        config = AutoConfig.from_pretrained(self.model_name, num_labels=2)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=config)\n",
    "\n",
    "        if self.freeze_encoder:\n",
    "            for name, p in model.named_parameters():\n",
    "                if \"classifier\" not in name: p.requires_grad = False\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=self.outdir,\n",
    "            learning_rate=self.lr,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size,\n",
    "            num_train_epochs=self.epochs,\n",
    "            weight_decay=self.weight_decay,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            warmup_ratio=0.1,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            fp16=self.fp16,\n",
    "            report_to=[]\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),\n",
    "            compute_metrics=self.compute_metrics(),\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.save_model(os.path.join(self.outdir, \"best_model\"))\n",
    "\n",
    "        val_metrics = trainer.evaluate(val_ds)\n",
    "        ind_metrics = trainer.evaluate(ind_ds)\n",
    "\n",
    "        return val_metrics, ind_metrics\n",
    "\n",
    "    def extract_embeddings(self, fasta_path, pooling=\"mean\", layer=None):\n",
    "        df = load_dataset_from_fasta(fasta_path)\n",
    "        model = AutoModel.from_pretrained(self.model_name, output_hidden_states=True).eval()\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model.to(device)\n",
    "\n",
    "        vecs, headers, labels = [], df[\"header\"].tolist(), df[\"label\"].tolist()\n",
    "        for i in range(0, len(df), self.batch_size):\n",
    "            seqs = df[\"sequence\"].iloc[i:i+self.batch_size].tolist()\n",
    "            enc = self.tokenizer(seqs, truncation=True, padding=True,\n",
    "                                 max_length=self.max_length, return_tensors=\"pt\")\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            out = model(**enc)\n",
    "            hidden = out.hidden_states[layer] if layer is not None else out.last_hidden_state\n",
    "\n",
    "            if pooling == \"cls\":\n",
    "                vec = hidden[:, 0, :]\n",
    "            elif pooling == \"mean\":\n",
    "                mask = enc[\"attention_mask\"].unsqueeze(-1)\n",
    "                vec = (hidden * mask).sum(1) / mask.sum(1).clamp(min=1)\n",
    "            else:\n",
    "                vec = hidden.reshape(hidden.size(0), -1)\n",
    "            vecs.append(vec.cpu().detach().numpy())\n",
    "\n",
    "        X = np.concatenate(vecs, axis=0)\n",
    "        return X, pd.DataFrame({\"header\": headers, \"label\": labels})\n",
    "\n",
    "\n",
    "    # ---------- prediction ----------\n",
    "    def predict(self, fasta_path: str, threshold: float = 0.5, out_csv: Optional[str] = None) -> pd.DataFrame:\n",
    "        # allow unlabeled FASTA\n",
    "        df = load_dataset_from_fasta(fasta_path)\n",
    "        df.rename(columns={\"header\": \"seq_id\"}, inplace=True)\n",
    "        \n",
    "        ds = ESM2Dataset(df, self.tokenizer, self.max_length)\n",
    "        collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        # config = AutoConfig.from_pretrained(self.model_name, num_labels=2)\n",
    "        # model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        #         os.path.join(self.outdir, \"best_model\"), config=config\n",
    "        #     )\n",
    "\n",
    "        best_dir = os.path.join(self.outdir, \"best_model\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(best_dir)\n",
    "        model.eval()\n",
    "        \n",
    "        # model.predict = True\n",
    "        trainer = Trainer(model=model, tokenizer=self.tokenizer,\n",
    "            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer))\n",
    "        proba = trainer.predict(ds)\n",
    "        proba = proba.predictions[:, 1]\n",
    "        y_pred = (proba >= threshold).astype(int)\n",
    "        # y_pred = proba.argmax(axis=1)\n",
    "        # y_pred = np.argmax(proba.predictions, axis=1)\n",
    "        # df = self._maybe_unlabeled_df(fasta_path)\n",
    "        # df.rename(columns={\"header\": \"seq_id\"}, inplace=True)\n",
    "\n",
    "        # eff_max = self._effective_max_len()\n",
    "        # pred_ds = ESM2Dataset(df, self.tokenizer, eff_max)\n",
    "        # collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "\n",
    "        # best_dir = os.path.join(self.outdir, \"best_model\")\n",
    "        # model = AutoModelForSequenceClassification.from_pretrained(best_dir)\n",
    "        # model.eval()\n",
    "\n",
    "        # # trainer = Trainer(model=model, tokenizer=self.tokenizer, data_collator=collator)\n",
    "        # trainer = SafeTrainer(model=model, tokenizer=self.tokenizer,\n",
    "        #               data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer))\n",
    "        # out = trainer.predict(pred_ds)\n",
    "        # logits = out.predictions\n",
    "        # probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "        # proba = probs[:, 1]\n",
    "        # y_pred = (proba >= threshold).astype(int)\n",
    "\n",
    "        res = df.copy()\n",
    "        # ensure numeric labels or NaN\n",
    "        if \"label\" in res.columns:\n",
    "            res[\"label\"] = pd.to_numeric(res[\"label\"], errors=\"coerce\")\n",
    "        else:\n",
    "            res[\"label\"] = np.nan\n",
    "\n",
    "        res[\"proba\"] = proba\n",
    "        res[\"y_pred\"] = y_pred\n",
    "        res = res[[\"seq_id\", \"label\", \"proba\", \"y_pred\"]]\n",
    "        res.rename(columns={\"label\": \"y_true\"}, inplace=True)\n",
    "\n",
    "        # metrics if y_true present\n",
    "        if res[\"y_true\"].notna().all():\n",
    "            y_true = res[\"y_true\"].astype(int).values\n",
    "            try:\n",
    "                auc  = roc_auc_score(y_true, proba)\n",
    "                prau = average_precision_score(y_true, proba)\n",
    "            except Exception:\n",
    "                auc, prau = float(\"nan\"), float(\"nan\")\n",
    "            acc  = accuracy_score(y_true, y_pred)\n",
    "            mcc  = matthews_corrcoef(y_true, y_pred)\n",
    "            f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "            prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "            rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "            spec = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "            print(f\"Accuracy: {acc:.4f}\")\n",
    "            print(f\"AUC: {auc:.4f}\")\n",
    "            print(f\"PR AUC: {prau:.4f}\")\n",
    "            print(f\"MCC: {mcc:.4f}\")\n",
    "            print(f\"F1: {f1:.4f}\")\n",
    "            print(f\"Precision: {prec:.4f}\")\n",
    "            print(f\"Recall/Sensitivity: {rec:.4f}\")\n",
    "            print(f\"Specificity: {spec:.4f}\")\n",
    "        else:\n",
    "            print(\"No ground-truth labels detected; wrote probabilities only.\")\n",
    "\n",
    "        if out_csv is None:\n",
    "            out_csv = os.path.join(self.outdir, \"predictions.csv\")\n",
    "        res.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved predictions to: {out_csv}\")\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T21:30:38.882911Z",
     "iopub.status.busy": "2025-09-02T21:30:38.882598Z",
     "iopub.status.idle": "2025-09-02T21:35:43.541463Z",
     "shell.execute_reply": "2025-09-02T21:35:43.540688Z",
     "shell.execute_reply.started": "2025-09-02T21:30:38.882889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_FASTA = \"/kaggle/input/lbtope-ibce-final-dataset-features/new_dataset/Train-ibce.fasta\"\n",
    "IND_FASTA   = \"/kaggle/input/lbtope-ibce-final-dataset-features/new_dataset/Ind-ibce50_renamed.fasta\"\n",
    "\n",
    "pipeline = ESM2Pipeline(\n",
    "    model_name=\"facebook/esm2_t12_35M_UR50D\",\n",
    "    outdir=\"./esm2_run_64_t33_650M\",\n",
    "    max_length=64,\n",
    "    epochs=5,\n",
    "    batch_size=8,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Train + evaluate\n",
    "val_metrics, ind_metrics = pipeline.train_and_eval(TRAIN_FASTA, IND_FASTA)\n",
    "# print(val_metrics)\n",
    "# print(ind_metrics)\n",
    "df_ind = pd.DataFrame([val_metrics, ind_metrics], index=[1,2])\n",
    "df_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T21:47:21.851748Z",
     "iopub.status.busy": "2025-09-02T21:47:21.851422Z",
     "iopub.status.idle": "2025-09-02T21:47:28.962494Z",
     "shell.execute_reply": "2025-09-02T21:47:28.961882Z",
     "shell.execute_reply.started": "2025-09-02T21:47:21.851728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_ind = pipeline.predict(IND_FASTA, threshold=0.1, out_csv=\"ind_ibce_predictions.csv\")\n",
    "df_ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T21:48:54.415579Z",
     "iopub.status.busy": "2025-09-02T21:48:54.415274Z",
     "iopub.status.idle": "2025-09-02T21:48:58.788808Z",
     "shell.execute_reply": "2025-09-02T21:48:58.788037Z",
     "shell.execute_reply.started": "2025-09-02T21:48:54.415556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Test_clbe = \"//kaggle/input/lbtope-ibce-final-dataset-features/features/ind_clbe/Test_clbe_filtered.fasta\"\n",
    "df_ind = pipeline.predict(Test_clbe, threshold=0.25, out_csv=\"test_clbe_predictions.csv\")\n",
    "df_ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T21:49:20.077777Z",
     "iopub.status.busy": "2025-09-02T21:49:20.077239Z",
     "iopub.status.idle": "2025-09-02T21:49:23.499765Z",
     "shell.execute_reply": "2025-09-02T21:49:23.499214Z",
     "shell.execute_reply.started": "2025-09-02T21:49:20.077754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Test_abcpred = \"/kaggle/input/lbtope-ibce-final-dataset-features/features/ind_abcpred/abcpred_filtered.fasta\"\n",
    "df_ind = pipeline.predict(Test_abcpred, threshold=0.1, out_csv=\"test_abcpred_predictions.csv\")\n",
    "df_ind.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8095417,
     "sourceId": 12923874,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
