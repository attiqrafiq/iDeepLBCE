{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T17:22:54.126135Z",
     "iopub.status.busy": "2025-08-31T17:22:54.125858Z",
     "iopub.status.idle": "2025-08-31T17:22:58.355652Z",
     "shell.execute_reply": "2025-08-31T17:22:58.354918Z",
     "shell.execute_reply.started": "2025-08-31T17:22:54.126110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install keras-tuner --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-03T19:47:35.622551Z",
     "iopub.status.busy": "2025-09-03T19:47:35.622250Z",
     "iopub.status.idle": "2025-09-03T19:47:35.758164Z",
     "shell.execute_reply": "2025-09-03T19:47:35.757221Z",
     "shell.execute_reply.started": "2025-09-03T19:47:35.622530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, math, time, random, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(__doc__)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# from tensorflow.python.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Input, LSTM, Conv1D, MaxPool1D, BatchNormalization, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras_tuner.tuners import RandomSearch, Hyperband\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, classification_report, f1_score, precision_recall_curve, average_precision_score\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "import itertools\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T19:47:39.944506Z",
     "iopub.status.busy": "2025-09-03T19:47:39.944221Z",
     "iopub.status.idle": "2025-09-03T19:47:39.958525Z",
     "shell.execute_reply": "2025-09-03T19:47:39.957410Z",
     "shell.execute_reply.started": "2025-09-03T19:47:39.944477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_performace(test_num, y_prob, y_test):\n",
    "\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    # Metrics\n",
    "    _f1 = f1_score(y_test, y_pred)\n",
    "    _acc = accuracy_score(y_test, y_pred)\n",
    "    _auc = roc_auc_score(y_test, y_prob)\n",
    "    _mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    _cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = _cm.ravel()\n",
    "    \n",
    "    # Calculate Sensitivity\n",
    "    sens = TP / (TP + FN)\n",
    "    \n",
    "    # Calculate Specificity\n",
    "    spec = TN / (TN + FP)\n",
    "\n",
    "    acc = _acc #float(tp + tn) / test_num\n",
    "    precision = float(TP) / (TP + FP)\n",
    "    sensitivity = sens # float(tp) / (tp + fn)\n",
    "    specificity = spec # float(tn) / (tn + fp)\n",
    "    MCC = _mcc # (float(tp) * tn - fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    return acc, precision, sensitivity, specificity, MCC\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=True,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues, save_path=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    xmin, xmax = plt.xlim()  # return the current xlim\n",
    "    plt.xlim((xmin, xmax))  # set the xlim to xmin, xmax\n",
    "    plt.ylim(xmin, xmax)  # set the xlim to xmin, xmax\n",
    "\n",
    "    if normalize:\n",
    "        cm_norm = np.round((cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]*100),2)\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, f\"{cm[i, j]}\\n{cm_norm[i, j]}%\",\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "def plot_performance(train_loss,val_loss,train_acc,val_acc):\n",
    "    # plot train and validation loss across multiple runs\n",
    "    plt.plot(train_loss, color='blue', label='train')\n",
    "    plt.plot(val_loss, color='orange', label='validation')\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_acc, color='blue', label='train')\n",
    "    plt.plot(val_acc, color='orange', label='validation')\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()\n",
    "def transfer_label_from_prob(proba):\n",
    "    label = [1 if val >= 0.5 else 0 for val in proba]\n",
    "    return label\n",
    "def plot_roc_curve(labels, probality, legend_text, auc_tag=True, save_path=None):\n",
    "    # fpr2, tpr2, thresholds = roc_curve(labels, pred_y)\n",
    "    fpr, tpr, thresholds = roc_curve(labels, probality)  # probas_[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if auc_tag:\n",
    "        rects1 = plt.plot(fpr, tpr, label=legend_text + ' (AUC=%6.3f) ' % roc_auc)\n",
    "    else:\n",
    "        rects1 = plt.plot(fpr, tpr, label=legend_text)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Area under receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T19:47:43.536737Z",
     "iopub.status.busy": "2025-09-03T19:47:43.536400Z",
     "iopub.status.idle": "2025-09-03T19:47:43.553983Z",
     "shell.execute_reply": "2025-09-03T19:47:43.552427Z",
     "shell.execute_reply.started": "2025-09-03T19:47:43.536714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "str1='250904'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T19:48:03.979402Z",
     "iopub.status.busy": "2025-09-03T19:48:03.979138Z",
     "iopub.status.idle": "2025-09-03T19:48:03.984790Z",
     "shell.execute_reply": "2025-09-03T19:48:03.984022Z",
     "shell.execute_reply.started": "2025-09-03T19:48:03.979384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get splitted data for deep learning models models\n",
    "def getSplitDataSet(X, y, ratio=0.2):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    #split data into training and test data. \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ratio, random_state=245)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #########################\n",
    "### input feature set ###\n",
    "### #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T19:48:04.481228Z",
     "iopub.status.busy": "2025-09-03T19:48:04.480949Z",
     "iopub.status.idle": "2025-09-03T19:48:04.870844Z",
     "shell.execute_reply": "2025-09-03T19:48:04.869645Z",
     "shell.execute_reply.started": "2025-09-03T19:48:04.481204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "url_dataset ='features\\\\train_ibce\\\\PSTPP_train.csv' \n",
    "df_main = pd.read_csv(url_dataset, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T19:48:05.632616Z",
     "iopub.status.busy": "2025-09-03T19:48:05.632269Z",
     "iopub.status.idle": "2025-09-03T19:48:05.704483Z",
     "shell.execute_reply": "2025-09-03T19:48:05.703777Z",
     "shell.execute_reply.started": "2025-09-03T19:48:05.632592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y = df_main.iloc[:,:1].values\n",
    "X = df_main.iloc[:,1:].values\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = y = encoder.fit_transform(y.ravel())\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, _scaler = getSplitDataSet(X, y, ratio=0.2)\n",
    "joblib.dump(_scaler, f\"{str1}_standard_scalsr.pkl\")\n",
    "\n",
    "fea_dim = X_train.shape[1]\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset reshaping according to models requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T19:48:58.576224Z",
     "iopub.status.busy": "2025-09-03T19:48:58.575944Z",
     "iopub.status.idle": "2025-09-03T19:48:58.584902Z",
     "shell.execute_reply": "2025-09-03T19:48:58.582882Z",
     "shell.execute_reply.started": "2025-09-03T19:48:58.576204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reshaping for CNN models\n",
    "XTrainCNN =np.array(X_train).reshape(-1, fea_dim, 1)\n",
    "XTestCNN = np.array(X_test).reshape(-1, fea_dim, 1)\n",
    "print(\"XTrainCNN Shape\",XTrainCNN.shape)\n",
    "\n",
    "# Reshaping for RNN models\n",
    "XTrainRNN =np.array(X_train).reshape(-1, 1, fea_dim)\n",
    "XTestRNN = np.array(X_test).reshape(-1, 1, fea_dim)\n",
    "print(\"XTrainRNN Shape\",XTrainRNN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weights calculation to handle imbalicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T19:48:59.129957Z",
     "iopub.status.busy": "2025-09-03T19:48:59.129669Z",
     "iopub.status.idle": "2025-09-03T19:48:59.139258Z",
     "shell.execute_reply": "2025-09-03T19:48:59.138199Z",
     "shell.execute_reply.started": "2025-09-03T19:48:59.129937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "cw = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "cw_dict = {0: cw[0], 1: cw[1]}\n",
    "print(cw_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:31:09.599360Z",
     "iopub.status.busy": "2025-08-31T20:31:09.599029Z",
     "iopub.status.idle": "2025-08-31T20:31:09.607528Z",
     "shell.execute_reply": "2025-08-31T20:31:09.606975Z",
     "shell.execute_reply.started": "2025-08-31T20:31:09.599343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_shape=X_train.shape[1:]\n",
    "checkpoint_filepath = f'{str1}_FCNN.h5'\n",
    "callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", verbose=1, mode=\"max\", patience=100, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.5, patience=8, min_lr=1e-6, verbose=1),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
    "            ]\n",
    "def build_dnn_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Model Input\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    # Number of layers the dense and its l2 kernal_regularizer, the activation constant to 'relu', and dropout\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 2)):\n",
    "        model.add(Dense(units=hp.Int(f'dense_{i}_units', min_value=32, max_value=256, step=32), \n",
    "                        activation='relu', kernel_regularizer = regularizers.l2(hp.Choice(f'{i}_l2Reg', [1e-2, 1e-3, 1e-4]))))\n",
    "        model.add(Dropout(rate=hp.Choice(f'dropout_rate{i}', [0.2, 0.3, 0.4])))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Optimizer selection\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd']) \n",
    "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile model with tunable loss\n",
    "    loss_choice = hp.Choice('loss', ['binary_crossentropy'])\n",
    "    model.compile(optimizer=optimizer, loss=loss_choice, metrics=['accuracy', 'mae', 'AUC'])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T20:31:27.089704Z",
     "iopub.status.busy": "2025-08-31T20:31:27.089461Z",
     "iopub.status.idle": "2025-08-31T20:31:56.018600Z",
     "shell.execute_reply": "2025-08-31T20:31:56.018043Z",
     "shell.execute_reply.started": "2025-08-31T20:31:27.089686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_dnn_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=100,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # How many models to build and fit for each trial\n",
    "    directory='dlm-optim-train-models/dlm',\n",
    "    project_name='dnn_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train,\n",
    "                 epochs=200,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 batch_size=32,\n",
    "                 callbacks=callbacks,\n",
    "                 verbose=0,  # <- controls output verbosity\n",
    "                 class_weight=cw_dict)\n",
    "\n",
    "tuner.reload()\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")\n",
    "\n",
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "model.save(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:50:42.940618Z",
     "iopub.status.busy": "2025-08-31T21:50:42.940326Z",
     "iopub.status.idle": "2025-08-31T21:50:42.949759Z",
     "shell.execute_reply": "2025-08-31T21:50:42.949095Z",
     "shell.execute_reply.started": "2025-08-31T21:50:42.940597Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build model function for tuner (SimpleRNN)\n",
    "input_shape = XTrainRNN.shape[1:] # X_train.shape[1] #\n",
    "checkpoint_filepath = f'{str1}_RNN.h5'\n",
    "callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", verbose=1, mode=\"max\", patience=20, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.5, patience=8, min_lr=1e-6, verbose=1),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
    "            ]\n",
    "def build_simple_rnn_model(hp):\n",
    "    from tensorflow.keras import regularizers\n",
    "    from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "    from tensorflow.keras import Sequential\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "\n",
    "    model = Sequential()\n",
    "    # SimpleRNN block\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 2)):\n",
    "        model.add(SimpleRNN(\n",
    "            units=hp.Int(f'rnn_{i}_units', 32, 256, step=32),\n",
    "            activation='relu',\n",
    "\n",
    "            kernel_regularizer = regularizers.l2(hp.Choice(f'{i}_l2Reg', [1e-2, 1e-3, 1e-4])),\n",
    "            return_sequences=True  # last output only, so no Flatten needed\n",
    "        ))\n",
    "        \n",
    "        model.add(Dropout(rate=hp.Choice(f'dropout_rate{i}', [0.2, 0.3, 0.4])))        \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('dense_units', 32, 256, step=32), activation='relu', kernel_regularizer = regularizers.l2(hp.Choice(f'dense_l2Reg', [1e-2, 1e-3, 1e-4]))))\n",
    "    model.add(Dropout(hp.Float('dense_dropout', 0.2, 0.5, step=0.1)))\n",
    "    # Output\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Optimizer + LR\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Loss\n",
    "    loss_choice = hp.Choice('loss', ['binary_crossentropy'])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_choice, metrics=['accuracy', 'mae', 'AUC'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-31T21:51:03.276541Z",
     "iopub.status.busy": "2025-08-31T21:51:03.276267Z",
     "iopub.status.idle": "2025-08-31T21:51:37.052007Z",
     "shell.execute_reply": "2025-08-31T21:51:37.051411Z",
     "shell.execute_reply.started": "2025-08-31T21:51:03.276522Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_simple_rnn_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=100,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # How many models to build and fit for each trial\n",
    "    directory='dlm-optim-train-models/dlm',\n",
    "    project_name='rnn_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(XTrainRNN, y_train,\n",
    "                 epochs=200,\n",
    "                 validation_data=(XTestRNN, y_test),\n",
    "                 batch_size=32,\n",
    "                 callbacks=callbacks,\n",
    "                 verbose=0,  # <- controls output verbosity\n",
    "                 class_weight=cw_dict)\n",
    "\n",
    "tuner.reload()\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")\n",
    "    \n",
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "model.save(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T21:04:41.590880Z",
     "iopub.status.busy": "2025-08-31T21:04:41.590600Z",
     "iopub.status.idle": "2025-08-31T21:04:41.599662Z",
     "shell.execute_reply": "2025-08-31T21:04:41.598867Z",
     "shell.execute_reply.started": "2025-08-31T21:04:41.590861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build model function for tuner\n",
    "input_shape=XTrainRNN.shape[1:]\n",
    "checkpoint_filepath = f'{str1}_GRU.h5'\n",
    "callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", verbose=1, mode=\"max\", patience=20, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.5, patience=8, min_lr=1e-6, verbose=1),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
    "            ]\n",
    "def build_gru_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 2)):\n",
    "        model.add(GRU(units=hp.Int(f'gru_{i}_units', 32, 256, step=32),\n",
    "                  activation='relu',\n",
    "                  recurrent_activation='relu',\n",
    "                  # dropout=hp.Float(f'gru_{i}_dropout', 0.1, 0.5, step=0.1),\n",
    "                  kernel_regularizer = regularizers.l2(hp.Choice(f'{i}_l2Reg', [1e-2, 1e-3, 1e-4])),\n",
    "                    return_sequences=True  # last output only, so no Flatten needed\n",
    "                  ))\n",
    "        \n",
    "        model.add(Dropout(rate=hp.Choice(f'dropout_rate{i}', [0.2, 0.3, 0.4]))) # hp.Float(f'dropout_{i}_rate', min_value=0.2, max_value=0.4, step=0.1)\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('dense_units', 32, 256, step=32), activation='relu', kernel_regularizer = regularizers.l2(hp.Choice(f'dense_l2Reg', [1e-2, 1e-3, 1e-4]))))\n",
    "    model.add(Dropout(hp.Float('dense_dropout', 0.2, 0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Optimizer and learning rate\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Loss function\n",
    "    loss_choice = hp.Choice('loss', ['binary_crossentropy'])\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss_choice,\n",
    "                  metrics=['accuracy', 'mae', 'AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-31T21:04:43.607151Z",
     "iopub.status.busy": "2025-08-31T21:04:43.606422Z",
     "iopub.status.idle": "2025-08-31T21:05:19.593067Z",
     "shell.execute_reply": "2025-08-31T21:05:19.592446Z",
     "shell.execute_reply.started": "2025-08-31T21:04:43.607127Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_gru_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=100,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # How many models to build and fit for each trial\n",
    "    directory='dlm-optim-train-models/dlm',\n",
    "    project_name='gru_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(XTrainRNN, y_train,\n",
    "                 epochs=200,\n",
    "                 validation_data=(XTestRNN, y_test),\n",
    "                 batch_size=32,\n",
    "                 callbacks=callbacks,\n",
    "                 verbose=0,  # <- controls output verbosity\n",
    "                 class_weight=cw_dict)\n",
    "\n",
    "tuner.reload()\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")\n",
    "\n",
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "model.save(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:04:40.747184Z",
     "iopub.status.busy": "2025-08-31T22:04:40.746556Z",
     "iopub.status.idle": "2025-08-31T22:04:40.755241Z",
     "shell.execute_reply": "2025-08-31T22:04:40.754661Z",
     "shell.execute_reply.started": "2025-08-31T22:04:40.747161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_shape=XTrainRNN.shape[1:]\n",
    "checkpoint_filepath = f'{str1}_LSTM.h5'\n",
    "callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", verbose=1, mode=\"max\", patience=20, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.5, patience=8, min_lr=1e-6, verbose=1),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
    "            ]\n",
    "def build_lstm_model(hp):\n",
    "    model = Sequential()\n",
    "    kernel_regularizer = regularizers.l2(hp.Choice('l2_reg', [1e-2, 1e-3, 1e-4]))\n",
    "    # First LSTM layer\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 2)):\n",
    "        model.add(LSTM(units=hp.Int(f'lstm_{i}_units', 32, 256, step=32),\n",
    "                  activation='relu',\n",
    "                  recurrent_activation='relu',\n",
    "                  # dropout=hp.Float(f'gru_{i}_dropout', 0.1, 0.5, step=0.1),\n",
    "                  kernel_regularizer = regularizers.l2(hp.Choice(f'{i}_l2Reg', [1e-2, 1e-3, 1e-4])),\n",
    "                    return_sequences=True  # last output only, so no Flatten needed\n",
    "                  ))\n",
    "        \n",
    "        model.add(Dropout(rate=hp.Choice(f'dropout_rate{i}', [0.2, 0.3, 0.4]))) \n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('dense_units', 32, 256, step=32), activation='relu', kernel_regularizer = regularizers.l2(hp.Choice(f'dense_l2Reg', [1e-2, 1e-3, 1e-4]))))\n",
    "    model.add(Dropout(hp.Float('dense_dropout', 0.2, 0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Optimizer and learning rate\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Loss function\n",
    "    loss_choice = hp.Choice('loss', ['binary_crossentropy'])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_choice, metrics=['accuracy', 'mae', 'AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T22:04:46.082157Z",
     "iopub.status.busy": "2025-08-31T22:04:46.081458Z",
     "iopub.status.idle": "2025-08-31T22:05:20.118696Z",
     "shell.execute_reply": "2025-08-31T22:05:20.117875Z",
     "shell.execute_reply.started": "2025-08-31T22:04:46.082133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_lstm_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=100,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # How many models to build and fit for each trial\n",
    "    directory='dlm-optim-train-models/dlm',\n",
    "    project_name='lstm_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(XTrainRNN, y_train,\n",
    "                 epochs=200,\n",
    "                 validation_data=(XTestRNN, y_test),\n",
    "                 batch_size=32,\n",
    "                 callbacks=callbacks,\n",
    "                 verbose=0,  # <- controls output verbosity\n",
    "                 class_weight=cw_dict)\n",
    "tuner.reload()\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")\n",
    "\n",
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "model.save(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T19:51:05.012798Z",
     "iopub.status.busy": "2025-09-03T19:51:05.012484Z",
     "iopub.status.idle": "2025-09-03T19:51:05.021696Z",
     "shell.execute_reply": "2025-09-03T19:51:05.020857Z",
     "shell.execute_reply.started": "2025-09-03T19:51:05.012778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_shape= XTrainCNN.shape[1:] #[fea_dim, 1]#\n",
    "checkpoint_filepath = f'{str1}_CNN.h5'\n",
    "callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", verbose=1, mode=\"max\", patience=20, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.5, patience=8, min_lr=1e-6, verbose=1),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
    "            ]\n",
    "def build_cnn_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 2)):\n",
    "        model.add(Conv1D(filters=hp.Int(f'filters_{i}', 8, 64, step=4), \n",
    "                     kernel_size=hp.Int(f'kernel_size_{i}', 2, 8, step=1), \n",
    "                     activation='relu', padding='same',\n",
    "                     kernel_regularizer = regularizers.l2(hp.Choice(f'{i}_l2Reg', [1e-2, 1e-3, 1e-4])), \n",
    "                     ))\n",
    "        model.add(MaxPool1D(pool_size=2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dropout(rate=hp.Choice(f'dropout_rate{i}', [0.2, 0.3, 0.4]))) \n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('dense_units', 32, 256, step=32), activation='relu', kernel_regularizer = regularizers.l2(hp.Choice(f'dense_l2Reg', [1e-2, 1e-3, 1e-4]))))\n",
    "    model.add(Dropout(hp.Float('dense_dropout', 0.2, 0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Optimizer selection\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile model with tunable loss\n",
    "    loss_choice = hp.Choice('loss', ['binary_crossentropy'])\n",
    "    model.compile(optimizer=optimizer, loss=loss_choice, metrics=['accuracy', 'mae', 'AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T19:51:53.807079Z",
     "iopub.status.busy": "2025-09-03T19:51:53.806797Z",
     "iopub.status.idle": "2025-09-03T19:57:36.805012Z",
     "shell.execute_reply": "2025-09-03T19:57:36.803867Z",
     "shell.execute_reply.started": "2025-09-03T19:51:53.807062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_cnn_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=100,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # How many models to build and fit for each trial\n",
    "    directory='dlm-optim-train-models/dlm',\n",
    "    project_name='cnn_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "tuner.search(XTrainCNN, y_train,\n",
    "                 epochs=200,\n",
    "                 validation_data=(XTestCNN, y_test),\n",
    "                 batch_size=32,\n",
    "                 callbacks=callbacks,\n",
    "                 verbose=1,  # <- controls output verbosity\n",
    "                 class_weight=cw_dict)\n",
    "tuner.reload()\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")\n",
    "\n",
    "model = tuner.get_best_models(num_models=1)[0]\n",
    "model.save(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T09:28:46.044676Z",
     "iopub.status.busy": "2025-09-01T09:28:46.043997Z",
     "iopub.status.idle": "2025-09-01T09:28:46.058377Z",
     "shell.execute_reply": "2025-09-01T09:28:46.057586Z",
     "shell.execute_reply.started": "2025-09-01T09:28:46.044652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "defined_path = 'dlm-optim-train-models/dlm/'\n",
    "def predict_pipeline(\n",
    "    fasta_list, models_list, combine_feature=False, output_dir=\"results\"\n",
    "):\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Creating output directory: {output_dir}\")\n",
    "        os.makedirs(output_dir)\n",
    "    results = []  ### rename to results_train\n",
    "    model_preds = []\n",
    "\n",
    "    # Load Scalar to Scale features\n",
    "    scaler_path = f\"{defined_path}minmax_scaler.pkl\"\n",
    "    minmax_scaler = joblib.load(scaler_path)\n",
    "    print(\"Scaler loaded\")\n",
    "    \n",
    "    for fasta in fasta_list:\n",
    "\n",
    "        print(fasta)\n",
    "        file_name = fasta.split(\"/\")[-2] #os.path.splitext(fasta).split(\"/\")[-2]\n",
    "        print(file_name)\n",
    "        # Define file paths for training and independent feature files\n",
    "        \n",
    "        # url_dataset =\"features/ind_ibce/PSTPP_ind.csv\" #\"lbtope_bcell_variable/new_lbtope_ind/ind-ibce50_1_pstpp.csv\" #\"features/ind_abcpred/PSTPP_abcpred.csv\" # \"lbtope_bcell_variable/new_lbtope_ind/abcpred_f50_pstpp.csv\" # 'lbtope_bcell_variable/lbtope_all_pstpp_new.csv' #'/kaggle/input/bcell-lbtope-features/lbtope_all_PSTPP.csv' # 'Benchmark Dataset\\PSCV_all_new.csv'#'hm5c_All_XGBModel.csv' # '_PSCV_Features.csv' # 'Benchmark Dataset\\PSCV_desc_new.csv' #\n",
    "        fv_set = pd.read_csv(fasta, header=0)\n",
    "        print(f\"\\n=== Processing: {file_name} having shape {fv_set.shape} ===\")\n",
    "\n",
    "        for model_name in models_list:\n",
    "            print(f\"Training {model_name} on {file_name}...\")\n",
    "            # print(fv_set.head(10))\n",
    "            X_test, y_test = fv_set.drop(columns=[\"label\"]), fv_set[\"label\"]\n",
    "\n",
    "            if X_test.isnull().values.any():\n",
    "                count_nan = X_test.isnull().sum().sum()\n",
    "                print(\n",
    "                    f\"X_test contains {count_nan} NaN values. Filling NaN values with 0.\"\n",
    "                )\n",
    "                X_test = X_test.fillna(0)\n",
    "\n",
    "            # Ensure labels are integers\n",
    "            y_test = y_test.astype(int)\n",
    "\n",
    "            # Label encode the labels\n",
    "            y_test = LabelEncoder().fit_transform(y_test)\n",
    "                \n",
    "            X_test = minmax_scaler.transform(X_test)\n",
    "\n",
    "            if model_name.upper() == \"CNN\":\n",
    "                X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "            elif model_name.upper() in [\"RNN\", \"LSTM\", \"GRU\"]:\n",
    "                X_test = X_test.reshape(-1, 1, X_test.shape[1])\n",
    "\n",
    "            # Print shapes\n",
    "            print(\n",
    "                f\"Validation set shape: {X_test.shape}, Labels shape: {y_test.shape}\"\n",
    "                )\n",
    "\n",
    "            # Load model model = tf.keras.models.load_model(f'{str1}_FCNN.h5')\n",
    "            file_path = f\"{defined_path}{str1}_{model_name}.h5\"\n",
    "            model = tf.keras.models.load_model(file_path)\n",
    "            print(f\"{file_path} Model Loaded\")\n",
    "\n",
    "            # Predict\n",
    "            y_pred_prob = model.predict(X_test).ravel()\n",
    "            y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "            # Metrics\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            acc = accuracy_score(\n",
    "                    y_test,\n",
    "                    y_pred,\n",
    "                )\n",
    "            _auc = roc_auc_score(y_test, y_pred_prob)\n",
    "            mAP = average_precision_score(y_test, y_pred_prob)\n",
    "            mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            # import matplotlib as mpl\n",
    "\n",
    "            # cmap = mpl.colormaps[\"jet\"]\n",
    "            # cm_path = os.path.join(output_dir, f\"{model_name}_CM_Internal Independent Set.svg\")\n",
    "            if file_name.lower().__contains__(\"ibce\"):\n",
    "                cm_title = (\n",
    "                        f\"{model_name} Confusion Matrix on the Internal Set\"\n",
    "                    )\n",
    "                cm_path = os.path.join(\n",
    "                        output_dir, f\"{model_name}_CM_Internal Set.svg\"\n",
    "                    )\n",
    "            if file_name.lower().__contains__(\"clbe\"):\n",
    "                cm_title = (\n",
    "                        f\"{model_name} Confusion Matrix on the External Set I\"\n",
    "                )\n",
    "                cm_path = os.path.join(\n",
    "                        output_dir, f\"{model_name}_CM_External Set I.svg\"\n",
    "                    )\n",
    "            if file_name.lower().__contains__(\"abcpred\"):\n",
    "                cm_title = (\n",
    "                        f\"{model_name} Confusion Matrix on the External Set II\"\n",
    "                    )\n",
    "                cm_path = os.path.join(\n",
    "                        output_dir, f\"{model_name}_CM_External Set II.svg\"\n",
    "                    )\n",
    "            plot_confusion_matrix(\n",
    "                    cm,\n",
    "                    classes=[\"nBCE\", \"pBCE\"],\n",
    "                    title=cm_title,\n",
    "                    # cmap=cmap,\n",
    "                    normalize=True,\n",
    "                    save_path=cm_path,\n",
    "                )\n",
    "            # plot_confusion_matrix(cm, classes=[0, 1], title=f\"{model_name}_{file_name}\", normalize=True, output_dir=output_dir)\n",
    "            TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "            # Calculate Sensitivity\n",
    "            sens = TP / (TP + FN)\n",
    "\n",
    "            # Calculate Specificity\n",
    "            spec = TN / (TN + FP)\n",
    "\n",
    "            # Save results\n",
    "            model_preds.append(\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Model\": model_name,\n",
    "                        \"Dataset\": file_name,\n",
    "                        \"orig_idx\": fv_set.index,  # position in original X_all\n",
    "                        \"y_true\": y_test.astype(int).ravel(),\n",
    "                        \"proba\": y_pred_prob.astype(float).ravel(),\n",
    "                        \"y_pred\": y_pred.astype(int).ravel(),\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Save results\n",
    "            results.append(\n",
    "                {\n",
    "                    # \"Feature_Set\": feat_name,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Dataset\": file_name,\n",
    "                    \"Accuracy\": acc,\n",
    "                    \"F1_Score\": f1,\n",
    "                    \"Sensitivity\": sens,\n",
    "                    \"Specificity\": spec,\n",
    "                    \"mAP\": mAP,\n",
    "                    \"AuROC\": _auc,\n",
    "                    \"MCC\": mcc,\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    # # Save results CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_csv_path = os.path.join(output_dir, f\"{file_name}_ind_results.csv\")\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "    # Save results CSV\n",
    "    model_preds_df = pd.concat(\n",
    "        model_preds, ignore_index=True\n",
    "    )  # pd.DataFrame(model_preds)\n",
    "    results_csv_path = os.path.join(output_dir, f\"{file_name}_prediction.csv\")\n",
    "    model_preds_df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "    print(f\"\\nResults saved to {results_csv_path}\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T09:28:52.173346Z",
     "iopub.status.busy": "2025-09-01T09:28:52.173060Z",
     "iopub.status.idle": "2025-09-01T09:29:12.357185Z",
     "shell.execute_reply": "2025-09-01T09:29:12.356557Z",
     "shell.execute_reply.started": "2025-09-01T09:28:52.173324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ind_fasta_list = [\n",
    "        \"/kaggle/input/lbtope-ibce-final-dataset-features/features/ind_ibce/PSTPP_ind.csv\",\n",
    "        \"/kaggle/input/lbtope-ibce-final-dataset-features/features/ind_clbe/PSTPP_ind.csv\",\n",
    "        \"/kaggle/input/lbtope-ibce-final-dataset-features/features/ind_abcpred/PSTPP_ind.csv\",\n",
    "    ]  \n",
    "models_list = [\"CNN\", \"FCNN\", \"GRU\", \"RNN\", \"LSTM\"]\n",
    "\n",
    "predict_results_df = predict_pipeline(\n",
    "            ind_fasta_list,\n",
    "            models_list,\n",
    "            combine_feature=True,\n",
    "            output_dir=\"results/\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7450129,
     "sourceId": 12311303,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7539526,
     "sourceId": 12510087,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8095417,
     "sourceId": 12923874,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8181426,
     "sourceId": 12929270,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
