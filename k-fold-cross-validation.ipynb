{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed64814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:36:41.485446Z",
     "iopub.status.busy": "2025-08-25T19:36:41.485161Z",
     "iopub.status.idle": "2025-08-25T19:36:51.551948Z",
     "shell.execute_reply": "2025-08-25T19:36:51.550636Z"
    },
    "papermill": {
     "duration": 10.076918,
     "end_time": "2025-08-25T19:36:51.553802",
     "exception": false,
     "start_time": "2025-08-25T19:36:41.476884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install biopython -q\n",
    "! pip install keras-tuner --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07151040",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-25T19:36:51.568345Z",
     "iopub.status.busy": "2025-08-25T19:36:51.567749Z",
     "iopub.status.idle": "2025-08-25T19:37:19.522045Z",
     "shell.execute_reply": "2025-08-25T19:37:19.521133Z"
    },
    "papermill": {
     "duration": 27.96312,
     "end_time": "2025-08-25T19:37:19.523628",
     "exception": false,
     "start_time": "2025-08-25T19:36:51.560508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import UMAP as umap\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "import math, time, random, datetime\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    accuracy_score, recall_score, \n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.metrics import r2_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, recall_score  # ADD\n",
    "from statsmodels.stats.contingency_tables import mcnemar  # ADD\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers\n",
    "import joblib\n",
    "import itertools\n",
    "\n",
    "# >>> ADDED\n",
    "import json, time\n",
    "# >>> New Added\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, matthews_corrcoef, precision_score, recall_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.python.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Input, LSTM, Conv1D, MaxPool1D, BatchNormalization, Flatten, Dropout\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras_tuner.tuners import RandomSearch, Hyperband\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dataset ='features\\\\train_ibce\\\\PSTPP_train.csv' \n",
    "df_main = pd.read_csv(url_dataset, header=0)\n",
    "\n",
    "y = df_main.iloc[:,:1].values\n",
    "X = df_main.iloc[:,1:].values\n",
    "\n",
    "fea_dim = X_train.shape[1]\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Reshaping for CNN models\n",
    "XTrainCNN =np.array(X_train).reshape(-1, fea_dim, 1)\n",
    "XTestCNN = np.array(X_test).reshape(-1, fea_dim, 1)\n",
    "print(\"XTrainCNN Shape\",XTrainCNN.shape)\n",
    "\n",
    "# Reshaping for RNN models\n",
    "XTrainRNN =np.array(X_train).reshape(-1, 1, fea_dim)\n",
    "XTestRNN = np.array(X_test).reshape(-1, 1, fea_dim)\n",
    "print(\"XTrainRNN Shape\",XTrainRNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fbaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1='250904'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198bac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:19.550098Z",
     "iopub.status.busy": "2025-08-25T19:37:19.549481Z",
     "iopub.status.idle": "2025-08-25T19:37:19.559739Z",
     "shell.execute_reply": "2025-08-25T19:37:19.558954Z"
    },
    "papermill": {
     "duration": 0.01864,
     "end_time": "2025-08-25T19:37:19.561203",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.542563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape=X_train.shape[1:]\n",
    "checkpoint_filepath = f'{str1}_FCNN.h5'\n",
    "callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", verbose=1, mode=\"max\", patience=100, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.5, patience=8, min_lr=1e-6, verbose=1),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
    "            ]\n",
    "def build_dnn_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Model Input\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    # Number of layers the dense and its l2 kernal_regularizer, the activation constant to 'relu', and dropout\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 2)):\n",
    "        model.add(Dense(units=hp.Int(f'dense_{i}_units', min_value=32, max_value=256, step=32), \n",
    "                        activation='relu', kernel_regularizer = regularizers.l2(hp.Choice(f'{i}_l2Reg', [1e-2, 1e-3, 1e-4]))))\n",
    "        model.add(Dropout(rate=hp.Choice(f'dropout_rate{i}', [0.2, 0.3, 0.4])))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Optimizer selection\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd']) \n",
    "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile model with tunable loss\n",
    "    loss_choice = hp.Choice('loss', ['binary_crossentropy'])\n",
    "    model.compile(optimizer=optimizer, loss=loss_choice, metrics=['accuracy', 'mae', 'AUC'])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74029f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:19.575058Z",
     "iopub.status.busy": "2025-08-25T19:37:19.574700Z",
     "iopub.status.idle": "2025-08-25T19:37:19.583197Z",
     "shell.execute_reply": "2025-08-25T19:37:19.582415Z"
    },
    "papermill": {
     "duration": 0.01682,
     "end_time": "2025-08-25T19:37:19.584473",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.567653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build model function for tuner (SimpleRNN)\n",
    "input_shape = XTrainRNN.shape[1:]\n",
    "checkpoint_filepath = f'{str1}_RNN.h5'\n",
    "callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", verbose=1, mode=\"max\", patience=20, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.5, patience=8, min_lr=1e-6, verbose=1),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
    "            ]\n",
    "def build_simple_rnn_model(hp):\n",
    "    from tensorflow.keras import regularizers\n",
    "    from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "    from tensorflow.keras import Sequential\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "\n",
    "    model = Sequential()\n",
    "    # SimpleRNN block\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 2)):\n",
    "        model.add(SimpleRNN(\n",
    "            units=hp.Int(f'rnn_{i}_units', 32, 256, step=32),\n",
    "            activation='relu',\n",
    "\n",
    "            kernel_regularizer = regularizers.l2(hp.Choice(f'{i}_l2Reg', [1e-2, 1e-3, 1e-4])),\n",
    "            return_sequences=True  # last output only, so no Flatten needed\n",
    "        ))\n",
    "        \n",
    "        model.add(Dropout(rate=hp.Choice(f'dropout_rate{i}', [0.2, 0.3, 0.4])))        \n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Output\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Optimizer + LR\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd']) \n",
    "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Loss\n",
    "    loss_choice = hp.Choice('loss', ['binary_crossentropy'])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_choice, metrics=['accuracy', 'mae', 'AUC'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76630e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:19.598391Z",
     "iopub.status.busy": "2025-08-25T19:37:19.597791Z",
     "iopub.status.idle": "2025-08-25T19:37:19.605354Z",
     "shell.execute_reply": "2025-08-25T19:37:19.604617Z"
    },
    "papermill": {
     "duration": 0.015867,
     "end_time": "2025-08-25T19:37:19.606639",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.590772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build model function for tuner\n",
    "input_shape=XTrainRNN.shape[1:]\n",
    "checkpoint_filepath = f'{str1}_GRU.h5'\n",
    "callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", verbose=1, mode=\"max\", patience=20, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.5, patience=8, min_lr=1e-6, verbose=1),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
    "            ]\n",
    "def build_gru_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 2)):\n",
    "        model.add(GRU(units=hp.Int(f'gru_{i}_units', 32, 256, step=32),\n",
    "                  activation='relu',\n",
    "                  recurrent_activation='relu',\n",
    "                  # dropout=hp.Float(f'gru_{i}_dropout', 0.1, 0.5, step=0.1),\n",
    "                  kernel_regularizer = regularizers.l2(hp.Choice(f'{i}_l2Reg', [1e-2, 1e-3, 1e-4])),\n",
    "                    return_sequences=True  # last output only, so no Flatten needed\n",
    "                  ))\n",
    "        \n",
    "        model.add(Dropout(rate=hp.Choice(f'dropout_rate{i}', [0.2, 0.3, 0.4]))) \n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('dense_units', 32, 256, step=32), activation='relu', kernel_regularizer = regularizers.l2(hp.Choice(f'dense_l2Reg', [1e-2, 1e-3, 1e-4]))))\n",
    "    model.add(Dropout(hp.Float('dense_dropout', 0.2, 0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Optimizer and learning rate\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Loss function\n",
    "    loss_choice = hp.Choice('loss', ['binary_crossentropy'])\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss_choice,\n",
    "                  metrics=['accuracy', 'mae', 'AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd29d63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:19.620905Z",
     "iopub.status.busy": "2025-08-25T19:37:19.620610Z",
     "iopub.status.idle": "2025-08-25T19:37:19.628894Z",
     "shell.execute_reply": "2025-08-25T19:37:19.628202Z"
    },
    "papermill": {
     "duration": 0.016623,
     "end_time": "2025-08-25T19:37:19.630219",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.613596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape=XTrainRNN.shape[1:]\n",
    "checkpoint_filepath = f'{str1}_LSTM.h5'\n",
    "callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", verbose=1, mode=\"max\", patience=20, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.5, patience=8, min_lr=1e-6, verbose=1),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
    "            ]\n",
    "def build_lstm_model(hp):\n",
    "    model = Sequential()\n",
    "    kernel_regularizer = regularizers.l2(hp.Choice('l2_reg', [1e-2, 1e-3, 1e-4]))\n",
    "    # First LSTM layer\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 2)):\n",
    "        model.add(LSTM(units=hp.Int(f'lstm_{i}_units', 32, 256, step=32),\n",
    "                  activation='relu',\n",
    "                  recurrent_activation='relu',\n",
    "                  # dropout=hp.Float(f'gru_{i}_dropout', 0.1, 0.5, step=0.1),\n",
    "                  kernel_regularizer = regularizers.l2(hp.Choice(f'{i}_l2Reg', [1e-2, 1e-3, 1e-4])),\n",
    "                    return_sequences=True  # last output only, so no Flatten needed\n",
    "                  ))\n",
    "        \n",
    "        model.add(Dropout(rate=hp.Choice(f'dropout_rate{i}', [0.2, 0.3, 0.4]))) \n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('dense_units', 32, 256, step=32), activation='relu', kernel_regularizer = regularizers.l2(hp.Choice(f'dense_l2Reg', [1e-2, 1e-3, 1e-4]))))\n",
    "    model.add(Dropout(hp.Float('dense_dropout', 0.2, 0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Optimizer and learning rate\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Loss function\n",
    "    loss_choice = hp.Choice('loss', ['binary_crossentropy'])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_choice, metrics=['accuracy', 'mae', 'AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c462ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:19.644079Z",
     "iopub.status.busy": "2025-08-25T19:37:19.643655Z",
     "iopub.status.idle": "2025-08-25T19:37:19.653687Z",
     "shell.execute_reply": "2025-08-25T19:37:19.652935Z"
    },
    "papermill": {
     "duration": 0.01841,
     "end_time": "2025-08-25T19:37:19.654916",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.636506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape=XTrainCNN.shape[1:]\n",
    "checkpoint_filepath = f'{str1}_CNN.h5'\n",
    "callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", verbose=1, mode=\"max\", patience=20, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.5, patience=8, min_lr=1e-6, verbose=1),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
    "            ]\n",
    "def build_cnn_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 2)):\n",
    "        model.add(Conv1D(filters=hp.Int(f'filters_{i}', 8, 64, step=4), \n",
    "                     kernel_size=hp.Int(f'kernel_size_{i}', 2, 8, step=1), \n",
    "                     activation='relu', padding='same',\n",
    "                     kernel_regularizer = regularizers.l2(hp.Choice(f'{i}_l2Reg', [1e-2, 1e-3, 1e-4])), \n",
    "                     ))\n",
    "        model.add(MaxPool1D(pool_size=2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dropout(rate=hp.Choice(f'dropout_rate{i}', [0.2, 0.3, 0.4]))) \n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('dense_units', 32, 256, step=32), activation='relu', kernel_regularizer = regularizers.l2(hp.Choice(f'dense_l2Reg', [1e-2, 1e-3, 1e-4]))))\n",
    "    model.add(Dropout(hp.Float('dense_dropout', 0.2, 0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Optimizer selection\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile model with tunable loss\n",
    "    loss_choice = hp.Choice('loss', ['binary_crossentropy'])\n",
    "    model.compile(optimizer=optimizer, loss=loss_choice, metrics=['accuracy', 'mae', 'AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea514f5",
   "metadata": {
    "papermill": {
     "duration": 0.005799,
     "end_time": "2025-08-25T19:37:19.667065",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.661266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2) Model builder factory\n",
    "### 3) Registry that points to your saved tuner directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a5d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:19.680257Z",
     "iopub.status.busy": "2025-08-25T19:37:19.679946Z",
     "iopub.status.idle": "2025-08-25T19:37:19.685573Z",
     "shell.execute_reply": "2025-08-25T19:37:19.684882Z"
    },
    "papermill": {
     "duration": 0.013701,
     "end_time": "2025-08-25T19:37:19.686700",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.672999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map each architecture to its tuner storage (directory + project_name) and build fn\n",
    "TUNER_REGISTRY = {\n",
    "    \"fcnn\": { # /kaggle/input/dlm-optim-train-models-bcells\n",
    "        \"directory\": \"dlm-optim-train-models/dlm\",\n",
    "        \"project_name\": \"dnn_tuning\",\n",
    "        \"build_fn\": build_dnn_model,\n",
    "        \"objective\": \"val_accuracy\",\n",
    "    },\n",
    "    # EXAMPLES — fill these with your real paths/names:\n",
    "    \"cnn\": {\n",
    "        \"directory\": \"dlm-optim-train-models/dlm\",\n",
    "        \"project_name\": \"cnn_tuning\",\n",
    "        \"build_fn\": build_cnn_model,      # you define above\n",
    "        \"objective\": \"val_accuracy\",\n",
    "    },\n",
    "    \"rnn\": {\n",
    "        \"directory\": \"dlm-optim-train-models/dlm\",\n",
    "        \"project_name\": \"rnn_tuning\",\n",
    "        \"build_fn\": build_simple_rnn_model,\n",
    "        \"objective\": \"val_accuracy\",\n",
    "    },\n",
    "    \"gru\": {\n",
    "        \"directory\": \"dlm-optim-train-models/dlm\",\n",
    "        \"project_name\": \"gru_tuning\",\n",
    "        \"build_fn\": build_gru_model,\n",
    "        \"objective\": \"val_accuracy\",\n",
    "    },\n",
    "    \"lstm\": {\n",
    "        \"directory\": \"dlm-optim-train-models/dlm\",\n",
    "        \"project_name\": \"lstm_tuning\",\n",
    "        \"build_fn\": build_lstm_model,\n",
    "        \"objective\": \"val_accuracy\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c4f09",
   "metadata": {
    "papermill": {
     "duration": 0.005745,
     "end_time": "2025-08-25T19:37:19.698625",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.692880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4) Input preparation helper (shape per architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d99036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:19.711733Z",
     "iopub.status.busy": "2025-08-25T19:37:19.711425Z",
     "iopub.status.idle": "2025-08-25T19:37:19.717554Z",
     "shell.execute_reply": "2025-08-25T19:37:19.716479Z"
    },
    "papermill": {
     "duration": 0.014251,
     "end_time": "2025-08-25T19:37:19.718845",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.704594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_input_by_arch(arch, X_tr, X_va):\n",
    "    \"\"\"\n",
    "    Returns (X_tr_ready, X_va_ready) shaped per-arch.\n",
    "    Assumes X_tr, X_va are numpy arrays of shape (n, d) for tabular features.\n",
    "    \"\"\"\n",
    "    if arch.lower() == \"cnn\":\n",
    "        # 1D-CNN expects (n, timesteps, channels)\n",
    "        X_tr = X_tr[..., np.newaxis]\n",
    "        X_va = X_va[..., np.newaxis]\n",
    "    elif arch.lower() in {\"rnn\", \"gru\", \"lstm\"}:\n",
    "        # RNN/GRU/LSTM expect (n, timesteps, features_per_step)\n",
    "        # Treat each feature as a \"time step\" with 1 channel (simple baseline)\n",
    "        X_tr = X_tr.reshape((X_tr.shape[0], X_tr.shape[1], 1))\n",
    "        X_va = X_va.reshape((X_va.shape[0], X_va.shape[1], 1))\n",
    "    else:\n",
    "        # DNN: keep as (n, d)\n",
    "        pass\n",
    "    return X_tr, X_va"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7eb4b4",
   "metadata": {
    "papermill": {
     "duration": 0.005993,
     "end_time": "2025-08-25T19:37:19.731095",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.725102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5) Best‑model loader from a saved tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8ea16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:19.744336Z",
     "iopub.status.busy": "2025-08-25T19:37:19.743786Z",
     "iopub.status.idle": "2025-08-25T19:37:19.749514Z",
     "shell.execute_reply": "2025-08-25T19:37:19.748443Z"
    },
    "papermill": {
     "duration": 0.013834,
     "end_time": "2025-08-25T19:37:19.750838",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.737004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_model_from_tuner(arch, input_shape):\n",
    "    \"\"\"\n",
    "    Recreates the best model for 'arch' by reloading the tuner and building with best_hps.\n",
    "    \"\"\"\n",
    "    reg = TUNER_REGISTRY[arch.lower()]\n",
    "    # NOTE: overwrite=False so past trials are preserved\n",
    "    tuner = kt.RandomSearch(\n",
    "        hypermodel=lambda hp: reg[\"build_fn\"](hp, input_shape),\n",
    "        objective=reg[\"objective\"],\n",
    "        max_trials=1,            # not searching; just need the object to reload\n",
    "        executions_per_trial=1,\n",
    "        directory=reg[\"directory\"],\n",
    "        project_name=reg[\"project_name\"],\n",
    "        overwrite=False\n",
    "    )\n",
    "    tuner.reload()\n",
    "    best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    return model, best_hps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b42f923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:19.786101Z",
     "iopub.status.busy": "2025-08-25T19:37:19.785802Z",
     "iopub.status.idle": "2025-08-25T19:37:19.828517Z",
     "shell.execute_reply": "2025-08-25T19:37:19.827853Z"
    },
    "papermill": {
     "duration": 0.051586,
     "end_time": "2025-08-25T19:37:19.830040",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.778454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_features(features_list, fileName, type=\"fixed\"):\n",
    "    \"\"\"\n",
    "    Combines multiple feature lists into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        features_list: A list of feature lists (each list is a list of features).\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing all combined features.\n",
    "    \"\"\"\n",
    "    if fileName.lower().__contains__(\"train\"):\n",
    "        folderName = fileName.lower()\n",
    "        fileName = \"train\"\n",
    "    else:\n",
    "        folderName = fileName.lower()\n",
    "        fileName = \"ind\"\n",
    "\n",
    "    print(\"File Name \", fileName, \" Folder Name \", folderName)\n",
    "    col = [\"label\"]\n",
    "    labels_path = os.path.join(\n",
    "        folderName,\n",
    "        \"labels.csv\",\n",
    "    )\n",
    "    labels = pd.read_csv(labels_path, header=0)\n",
    "    combined_features = pd.DataFrame(labels)\n",
    "    for i in range(0, len(features_list)):\n",
    "        if features_list[i].upper() == \"AAC\":\n",
    "            filepath = os.path.join(\n",
    "                folderName,\n",
    "                features_list[i].upper() + \"_\" + fileName + \".csv\",\n",
    "            )  # features/variable/train/PSTPP_train.csv\n",
    "            aac_features = pd.read_csv(filepath, header=0)\n",
    "            aac_features = aac_features.drop(columns=[\"label\"])\n",
    "            col.extend(aac_features.columns.tolist())\n",
    "            if combined_features.shape[1] == 0:\n",
    "                combined_features = aac_features\n",
    "            else:\n",
    "                combined_features = pd.concat([combined_features, aac_features], axis=1)\n",
    "\n",
    "        if features_list[i].upper() == \"DPC\":\n",
    "            filepath = os.path.join(\n",
    "                folderName,\n",
    "                features_list[i].upper() + \"_\" + fileName + \".csv\",\n",
    "            )\n",
    "            _features = pd.read_csv(filepath, header=0)\n",
    "            _features = _features.drop(columns=[\"label\"])\n",
    "            col.extend(_features.columns.tolist())\n",
    "            if combined_features.shape[1] == 0:\n",
    "                combined_features = _features\n",
    "            else:\n",
    "                combined_features = pd.concat([combined_features, _features], axis=1)\n",
    "\n",
    "        if features_list[i].upper() == \"PAAC\":\n",
    "            filepath = os.path.join(\n",
    "                folderName,\n",
    "                features_list[i].upper() + \"_\" + fileName + \".csv\",\n",
    "            )\n",
    "            _features = pd.read_csv(filepath, header=0)\n",
    "            _features = _features.drop(columns=[\"label\"])\n",
    "            col.extend(_features.columns.tolist())\n",
    "            if combined_features.shape[1] == 0:\n",
    "                combined_features = _features\n",
    "            else:\n",
    "                combined_features = pd.concat([combined_features, _features], axis=1)\n",
    "\n",
    "        if features_list[i].upper() == \"APAAC\":\n",
    "            filepath = os.path.join(\n",
    "                folderName,\n",
    "                features_list[i].upper() + \"_\" + fileName + \".csv\",\n",
    "            )\n",
    "            _features = pd.read_csv(filepath, header=0)\n",
    "            _features = _features.drop(columns=[\"label\"])\n",
    "            col.extend(_features.columns.tolist())\n",
    "            if combined_features.shape[1] == 0:\n",
    "                combined_features = _features\n",
    "            else:\n",
    "                combined_features = pd.concat([combined_features, _features], axis=1)\n",
    "\n",
    "        if features_list[i].lower() == \"pcp\":\n",
    "            filepath = os.path.join(\n",
    "                folderName,\n",
    "                features_list[i].upper() + \"_\" + fileName + \".csv\",\n",
    "            )\n",
    "            _features = pd.read_csv(filepath, header=0)\n",
    "            _features = _features.drop(columns=[\"label\"])\n",
    "            col.extend(_features.columns.tolist())\n",
    "            if combined_features.shape[1] == 0:\n",
    "                combined_features = _features\n",
    "            else:\n",
    "                combined_features = pd.concat([combined_features, _features], axis=1)\n",
    "\n",
    "        if features_list[i].lower() == \"onehot\":\n",
    "            filepath = os.path.join(\n",
    "                folderName,\n",
    "                features_list[i].upper() + \"_\" + fileName + \".csv\",\n",
    "            )\n",
    "            _features = pd.read_csv(filepath, header=0)\n",
    "            _features = _features.drop(columns=[\"label\"])\n",
    "            col.extend(_features.columns.tolist())\n",
    "            if combined_features.shape[1] == 0:\n",
    "                combined_features = _features\n",
    "            else:\n",
    "                combined_features = pd.concat([combined_features, _features], axis=1)\n",
    "\n",
    "        if features_list[i].lower() == \"esm2\":\n",
    "            filepath = os.path.join(\n",
    "                folderName,\n",
    "                features_list[i].upper() + \"_\" + fileName + \".csv\",\n",
    "            )\n",
    "            esm2_features = pd.read_csv(filepath, header=0)\n",
    "            esm2_features = esm2_features.drop(columns=[\"label\"])\n",
    "            col.extend(esm2_features.columns.tolist())\n",
    "            if combined_features.shape[1] == 0:\n",
    "                combined_features = esm2_features\n",
    "            else:\n",
    "                combined_features = pd.concat(\n",
    "                    [combined_features, esm2_features], axis=1\n",
    "                )\n",
    "\n",
    "        if features_list[i].lower() == \"pstpp\":\n",
    "            filepath = os.path.join(\n",
    "                folderName,\n",
    "                features_list[i].upper() + \"_\" + fileName + \".csv\",\n",
    "            )\n",
    "            pstpp_features = pd.read_csv(filepath, header=0)\n",
    "            pstpp_features = pstpp_features.drop(columns=[\"label\"])\n",
    "            col.extend(pstpp_features.columns.tolist())\n",
    "            if combined_features.shape[1] == 0:\n",
    "                combined_features = pstpp_features\n",
    "            else:\n",
    "                combined_features = pd.concat(\n",
    "                    [combined_features, pstpp_features], axis=1\n",
    "                )\n",
    "\n",
    "        if features_list[i].lower() == \"f1\":\n",
    "            filepath = os.path.join(\n",
    "                folderName,\n",
    "                features_list[i].upper() + \".csv\",\n",
    "            )\n",
    "            print(f\"The file {filepath} is loading\")\n",
    "            pstpp_features = pd.read_csv(filepath, header=0)\n",
    "            pstpp_features = pstpp_features.drop(columns=[\"label\"])\n",
    "            col.extend(pstpp_features.columns.tolist())\n",
    "            if combined_features.shape[1] == 0:\n",
    "                combined_features = pstpp_features\n",
    "            else:\n",
    "                combined_features = pd.concat(\n",
    "                    [combined_features, pstpp_features], axis=1\n",
    "                )\n",
    "        if features_list[i].lower() == \"f2\":\n",
    "            filepath = os.path.join(\n",
    "                folderName,\n",
    "                features_list[i].upper() + \".csv\",\n",
    "            )\n",
    "            print(f\"The file {filepath} is loading\")\n",
    "            pstpp_features = pd.read_csv(filepath, header=0)\n",
    "            pstpp_features = pstpp_features.drop(columns=[\"label\"])\n",
    "            col.extend(pstpp_features.columns.tolist())\n",
    "            if combined_features.shape[1] == 0:\n",
    "                combined_features = pstpp_features\n",
    "            else:\n",
    "                combined_features = pd.concat(\n",
    "                    [combined_features, pstpp_features], axis=1\n",
    "                )\n",
    "        \n",
    "        if features_list[i].lower() == \"pssm\":\n",
    "            filepath = os.path.join(\n",
    "                folderName,\n",
    "                features_list[i].upper() + \"_\" + fileName + \".csv\",\n",
    "            )\n",
    "            pssm_features = pd.read_csv(filepath, header=0)\n",
    "            pssm_features = pssm_features.drop(columns=[\"label\"])\n",
    "            col.extend(pssm_features.columns.tolist())\n",
    "            if combined_features.shape[1] == 0:\n",
    "                combined_features = pssm_features\n",
    "            else:\n",
    "                combined_features = pd.concat(\n",
    "                    [combined_features, pssm_features], axis=1\n",
    "                )\n",
    "\n",
    "        combined_features.columns = col\n",
    "    return combined_features\n",
    "\n",
    "\n",
    "def best_thresholds(y_true, proba):\n",
    "    y_true = np.asarray(y_true).astype(int).ravel()\n",
    "    proba = np.asarray(proba).ravel()\n",
    "    ts = np.linspace(0.01, 0.99, 99)\n",
    "    best_f1, t_f1 = -1.0, 0.5\n",
    "    best_mcc, t_mcc = -1.0, 0.5\n",
    "    best_sens, t_sens = -1.0, 0.5\n",
    "    best_spec, t_spec = -1.0, 0.5\n",
    "    best_acc, t_acc = -1.0, 0.5\n",
    "    best_auc, t_auc = -1.0, 0.5\n",
    "    best_map, t_map = -1.0, 0.5\n",
    "    for t in ts:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        f1 = f1_score(y_true, pred, zero_division=0)\n",
    "        mcc = matthews_corrcoef(y_true, pred) if (pred.max() != pred.min()) else -1.0\n",
    "        acc = accuracy_score(y_true, pred)\n",
    "        sens = recall_score(y_true, pred)\n",
    "        _auc = roc_auc_score(y_true, pred, average=\"weighted\")\n",
    "        MAP = average_precision_score(y_true, pred, average=\"weighted\")\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, pred, labels=[1, 0]).ravel()\n",
    "        spec = tn / (tn + fp) if (tn + fp) else 0.0\n",
    "        if f1 > best_f1:\n",
    "            best_f1, t_f1 = f1, t\n",
    "        if mcc > best_mcc:\n",
    "            best_mcc, t_mcc = mcc, t\n",
    "        if acc > best_acc:\n",
    "            best_acc, t_acc = acc, t\n",
    "        if sens > best_sens:\n",
    "            best_sens, t_sens = sens, t\n",
    "        if spec > best_spec:\n",
    "            best_spec, t_spec = spec, t\n",
    "        if _auc > best_auc:\n",
    "            best_auc, t_auc = _auc, t\n",
    "        if MAP > best_map:\n",
    "            best_map, t_map = MAP, t\n",
    "    return {\n",
    "        \"t_f1\": float(t_f1),\n",
    "        \"best_f1\": float(best_f1),\n",
    "        \"t_mcc\": float(t_mcc),\n",
    "        \"best_mcc\": float(best_mcc),\n",
    "        \"t_acc\": float(t_acc),\n",
    "        \"best_acc\": float(best_acc),\n",
    "        \"t_spec\": float(t_spec),\n",
    "        \"best_spec\": float(best_spec),\n",
    "        \"t_sens\": float(t_sens),\n",
    "        \"best_sens\": float(best_sens),\n",
    "        \"t_auc\": float(t_auc),\n",
    "        \"best_auc\": float(best_auc),\n",
    "        \"t_map\": float(t_map),\n",
    "        \"best_map\": float(best_map),\n",
    "    }\n",
    "\n",
    "\n",
    "def _binary_metrics_from_probs(y_true, y_prob, cutoff=0.5):\n",
    "    results = best_thresholds(y_true, y_prob)\n",
    "    # cutoff = thrs[\"t_f1\"]\n",
    "    y_pred = (y_prob >= cutoff).astype(int)\n",
    "    acc = results[\"best_acc\"]  # accuracy_score(y_true, y_pred)\n",
    "    sens = recall_score(y_true, y_pred)  # results[\"best_sens\"]  #\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    spec = tn / (tn + fp) if (tn + fp) else 0.0  # results[\"best_spec\"]  #\n",
    "\n",
    "    f1 = results[\"best_f1\"]  # f1_score(y_true, y_pred)\n",
    "    mcc = results[\"best_mcc\"]  # matthews_corrcoef(y_true, y_pred)\n",
    "    try:\n",
    "        aucv = results[\"best_auc\"]  # roc_auc_score(y_true, y_prob)\n",
    "        ap = results[\n",
    "            \"best_map\"\n",
    "        ]  # average_precision_score(y_true, y_prob, average=\"weighted\")  # <- add new line\n",
    "    except ValueError:\n",
    "        aucv = float(\"nan\")\n",
    "        ap = float(\"nan\")  # <- add new line\n",
    "    return dict(\n",
    "        ACC=acc, SENS=sens, SPEC=spec, F1=f1, mAP=ap, MCC=mcc, AUC=aucv, y_pred=y_pred\n",
    "    )  # <- add new line\n",
    "\n",
    "\n",
    "def _mcnemar_from_folds(y_true_folds, pred_ref_folds, pred_other_folds):\n",
    "    \"\"\"Aggregate McNemar across folds by summing n01/n10, then one test.\"\"\"\n",
    "    n01 = n10 = 0\n",
    "    for y, pr, po in zip(y_true_folds, pred_ref_folds, pred_other_folds):\n",
    "        ref_correct = pr == y\n",
    "        other_correct = po == y\n",
    "        n01 += int(np.sum(ref_correct & ~other_correct))  # ref correct, other wrong\n",
    "        n10 += int(np.sum(~ref_correct & other_correct))  # ref wrong, other correct\n",
    "    if (n01 + n10) == 0:\n",
    "        return float(\"nan\"), n01, n10\n",
    "    exact = (n01 + n10) < 25\n",
    "    res = mcnemar([[0, n01], [n10, 0]], exact=exact, correction=not exact)\n",
    "    return float(res.pvalue), n01, n10\n",
    "\n",
    "\n",
    "def _cv_eval_one_model(\n",
    "    model_name, X, y, k=5, epochs=50, batch=32, cutoff=0.5, verbose=0, feat_name=\"ACC\", output_dir=\"results\"\n",
    "):\n",
    "    arch = model_name\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    metrics_per_fold, y_true_folds, y_proba_folds, y_pred_folds, val_idx_folds = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "\n",
    "\n",
    "    for fold, (tr, va) in enumerate(skf.split(X, y), start=1):\n",
    "        print(f\"Fold {fold} on {model_name}\")\n",
    "        Xtr_raw, Xva_raw = X[tr], X[va]\n",
    "        ytr, yva = y[tr].astype(int), y[va].astype(int)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        Xtr = scaler.fit_transform(Xtr_raw)\n",
    "        Xva = scaler.transform(Xva_raw)\n",
    "\n",
    "\n",
    "        X_tr, X_va = prepare_input_by_arch(arch, Xtr_raw, Xva_raw)\n",
    "\n",
    "        # Build best model from saved tuner\n",
    "        input_shape = X_tr.shape[1:]\n",
    "        model, best_hps = get_best_model_from_tuner(arch, input_shape)\n",
    "        \n",
    "        reduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            factor=0.5,\n",
    "            patience=8,\n",
    "            verbose=0,\n",
    "            # min_lr=2e-5,\n",
    "            min_lr=1e-6,\n",
    "        )\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0,\n",
    "        )\n",
    "        model_output_dir = \"models\"\n",
    "        os.makedirs(model_output_dir, exist_ok=True)\n",
    "        file_path = os.path.join(\n",
    "            model_output_dir, f\"{feat_name}_{model_name}.weights.h5\"\n",
    "        )\n",
    "        save_best_model = tf.keras.callbacks.ModelCheckpoint(\n",
    "            file_path,\n",
    "            monitor=\"val_accuracy\",\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            mode=\"max\",\n",
    "            save_weights_only=True,\n",
    "        )\n",
    "\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            train_dataset = (\n",
    "                tf.data.Dataset.from_tensor_slices((Xtr, ytr))\n",
    "                .shuffle(1000)\n",
    "                .map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.int32)))\n",
    "                .batch(32)\n",
    "                .prefetch(tf.data.AUTOTUNE)\n",
    "            )\n",
    "            test_dataset = (\n",
    "                tf.data.Dataset.from_tensor_slices((Xva, yva))\n",
    "                .map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.int32)))\n",
    "                .batch(32)\n",
    "                .prefetch(tf.data.AUTOTUNE)\n",
    "            )\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch,\n",
    "            verbose=verbose,\n",
    "            validation_data=test_dataset,\n",
    "            callbacks=[reduce_on_plateau, early_stop, save_best_model],\n",
    "        )\n",
    "        train_seconds = time.perf_counter() - t0\n",
    "        model.load_weights(file_path)\n",
    "\n",
    "        new_file_path = file_path.replace(\".weights.h5\", \".h5\")\n",
    "        model.save(new_file_path)\n",
    "\n",
    "        prob = model.predict(Xva, verbose=0).ravel()\n",
    "        # If met[\"y_pred\"] already exists, you can use it; otherwise define here:\n",
    "        y_pred = (prob >= cutoff).astype(int)\n",
    "\n",
    "        met = _binary_metrics_from_probs(yva, prob, cutoff=cutoff)\n",
    "\n",
    "        fold_metrics = {\n",
    "            k: met[k] for k in (\"ACC\", \"SENS\", \"SPEC\", \"F1\", \"mAP\", \"MCC\", \"AUC\")\n",
    "        }\n",
    "        fold_metrics[\"TIME_SEC\"] = train_seconds\n",
    "        fold_metrics[\"TIME_PER_EPOCH_SEC\"] = train_seconds / len(\n",
    "            history.history[\"loss\"]\n",
    "        )\n",
    "        metrics_per_fold.append(fold_metrics)\n",
    "\n",
    "        # Keep 1-D arrays; no .T\n",
    "        y_true_folds.append(yva.copy())\n",
    "        y_proba_folds.append(prob.copy())\n",
    "        y_pred_folds.append(y_pred)  # or met[\"y_pred\"].astype(int)\n",
    "        val_idx_folds.append(va.copy())  # indices into original X\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    return metrics_per_fold, y_true_folds, y_pred_folds, y_proba_folds, val_idx_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13f154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:19.843738Z",
     "iopub.status.busy": "2025-08-25T19:37:19.843223Z",
     "iopub.status.idle": "2025-08-25T19:37:19.852267Z",
     "shell.execute_reply": "2025-08-25T19:37:19.851517Z"
    },
    "papermill": {
     "duration": 0.017292,
     "end_time": "2025-08-25T19:37:19.853623",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.836331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cv_train_eval_with_tuned_model(\n",
    "    arch, X, y, k=5, epochs=200, batch_size=32, verbose=0, cutoff=0.5, callbacks=None, random_state=42\n",
    "):\n",
    "    if callbacks is None: callbacks = []\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "\n",
    "    metrics_per_fold = []\n",
    "    y_true_folds, y_pred_folds, y_proba_folds = [], [], []\n",
    "\n",
    "    for fold, (tr, va) in enumerate(skf.split(X, y), start=1):\n",
    "        print(f\"[{arch.upper()}] Fold {fold}/{k}\")\n",
    "\n",
    "        X_tr_raw, X_va_raw = X[tr], X[va]\n",
    "        y_tr, y_va = y[tr], y[va]\n",
    "\n",
    "        # Shape per-arch\n",
    "        X_tr, X_va = prepare_input_by_arch(arch, X_tr_raw, X_va_raw)\n",
    "\n",
    "        # Build best model from saved tuner\n",
    "        input_shape = X_tr.shape[1:]\n",
    "        model, best_hps = get_best_model_from_tuner(arch, input_shape)\n",
    "\n",
    "        # Fit\n",
    "        hist = model.fit(\n",
    "            X_tr, y_tr,\n",
    "            validation_data=(X_va, y_va),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Predict on validation fold\n",
    "        proba = model.predict(X_va).ravel()\n",
    "        pred = (proba >= cutoff).astype(int)\n",
    "\n",
    "        # Collect fold-wise predictions\n",
    "        y_true_folds.append(y_va.copy())\n",
    "        y_pred_folds.append(pred.copy())\n",
    "        y_proba_folds.append(proba.copy())\n",
    "\n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_va, pred)\n",
    "        sens = recall_score(y_va, pred, zero_division=0)                 # TPR\n",
    "        spec = recall_score(1 - y_va, 1 - pred, zero_division=0)         # TNR\n",
    "        f1   = f1_score(y_va, pred, zero_division=0)\n",
    "        mcc  = matthews_corrcoef(y_va, pred)\n",
    "        try:\n",
    "            auc = roc_auc_score(y_va, proba)\n",
    "        except Exception:\n",
    "            auc = np.nan\n",
    "\n",
    "        metrics_per_fold.append({\n",
    "            \"fold\": fold,\n",
    "            \"ACC\": acc, \"SENS\": sens, \"SPEC\": spec, \"F1\": f1, \"MCC\": mcc, \"AUC\": auc\n",
    "        })\n",
    "\n",
    "        print(f\"  ACC={acc:.4f} SENS={sens:.4f} SPEC={spec:.4f} F1={f1:.4f} MCC={mcc:.4f} AUC={auc:.4f}\")\n",
    "\n",
    "    return metrics_per_fold, y_true_folds, y_pred_folds, y_proba_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d4372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:19.867444Z",
     "iopub.status.busy": "2025-08-25T19:37:19.867146Z",
     "iopub.status.idle": "2025-08-25T19:37:19.885889Z",
     "shell.execute_reply": "2025-08-25T19:37:19.885214Z"
    },
    "papermill": {
     "duration": 0.027304,
     "end_time": "2025-08-25T19:37:19.887139",
     "exception": false,
     "start_time": "2025-08-25T19:37:19.859835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ========== PIPELINE FUNCTION ==========\n",
    "def train_val_pipeline(\n",
    "    feature_list,\n",
    "    fasta_list,\n",
    "    models_list,\n",
    "    combine_features=False,\n",
    "    output_dir=\"results\",\n",
    "    cv_splits=5,\n",
    "    reference_model=\"CNN\",\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    cutoff=0.5,\n",
    "    verbose=0,\n",
    "):\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Creating output directory: {output_dir}\")\n",
    "        os.makedirs(output_dir)\n",
    "    results = []  ### rename to results_train\n",
    "\n",
    "    train_fasta = fasta_list[0]\n",
    "\n",
    "    file_name = os.path.splitext(train_fasta)[0].split(\"/\")[-1]\n",
    "\n",
    "    for feat_name in feature_list:\n",
    "        # Define file paths for training and independent feature files\n",
    "        print(f\"\\n=== Feature Set: {feat_name} ===\")\n",
    "\n",
    "        # Load features\n",
    "        if not combine_features:\n",
    "            fv_set = encode_features(\n",
    "                [feat_name],\n",
    "                type=\"\",\n",
    "                fileName=train_fasta.lower(),\n",
    "            )\n",
    "        else:\n",
    "            new_fea_list = []\n",
    "            # new_fea_list.append(_labels)\n",
    "            new_fea_list.extend(feature_list)\n",
    "            # new_fea_list.remove(feat_name)\n",
    "            fv_set = encode_features(feature_list, type=\"\", fileName=train_fasta.lower())\n",
    "\n",
    "        # print(fv_set.head(10))\n",
    "        # Build raw X,y and handle NaNs once per feature\n",
    "        fv = fv_set.copy()\n",
    "        fv = fv.fillna(0)\n",
    "        y_all = fv[\"label\"].astype(int).values\n",
    "        X_all = fv.drop(columns=[\"label\"]).values\n",
    "\n",
    "        # store CV outputs per model (for McNemar)\n",
    "        cv_metrics_by_model = {}  # name -> list of dicts per fold\n",
    "        y_true_folds_any = None  # will take from first model run\n",
    "\n",
    "        preds_by_model = {}\n",
    "        proba_by_model = {}\n",
    "        y_true_by_model = {}\n",
    "        va_idx_by_model = {}\n",
    "\n",
    "        for model_name in models_list:\n",
    "            print(f\"CV training {model_name} on {feat_name}...\")\n",
    "\n",
    "            met_folds, y_true_folds, y_pred_folds, y_proba_folds, val_idx_folds = (\n",
    "                _cv_eval_one_model(\n",
    "                    model_name,\n",
    "                    X_all,\n",
    "                    y_all,\n",
    "                    k=cv_splits,\n",
    "                    epochs=epochs,\n",
    "                    batch=batch_size,\n",
    "                    cutoff=cutoff,\n",
    "                    verbose=verbose,\n",
    "                    feat_name=feat_name,\n",
    "                    output_dir = output_dir\n",
    "                )\n",
    "            )\n",
    "            cv_metrics_by_model[model_name] = met_folds\n",
    "            preds_by_model[model_name] = y_pred_folds\n",
    "            proba_by_model[model_name] = y_proba_folds\n",
    "            y_true_by_model[model_name] = y_true_folds\n",
    "            va_idx_by_model[model_name] = val_idx_folds\n",
    "\n",
    "            if y_true_folds_any is None:\n",
    "                y_true_folds_any = y_true_folds\n",
    "\n",
    "        # === Aggregate per model: mean ± std (Table 1 style)\n",
    "        rows = []\n",
    "        for model_name in models_list:\n",
    "            mlist = cv_metrics_by_model[model_name]\n",
    "            ACC = np.array([m[\"ACC\"] for m in mlist])\n",
    "            SENS = np.array([m[\"SENS\"] for m in mlist])\n",
    "            SPEC = np.array([m[\"SPEC\"] for m in mlist])\n",
    "            F1 = np.array([m[\"F1\"] for m in mlist])\n",
    "            MAP = np.array([m[\"mAP\"] for m in mlist])  # <- add new line\n",
    "            MCC = np.array([m[\"MCC\"] for m in mlist])\n",
    "            AUCV = np.array([m[\"AUC\"] for m in mlist])\n",
    "            TSEC = np.array([m[\"TIME_SEC\"] for m in mlist])\n",
    "            TPE = np.array([m[\"TIME_PER_EPOCH_SEC\"] for m in mlist])\n",
    "\n",
    "            row = dict(\n",
    "                Feature_Set=feat_name,\n",
    "                Model=model_name,\n",
    "                ACC_Mean=ACC.mean(),\n",
    "                ACC_Std=ACC.std(ddof=1),\n",
    "                Sens_Mean=SENS.mean(),\n",
    "                Sens_Std=SENS.std(ddof=1),\n",
    "                Spec_Mean=SPEC.mean(),\n",
    "                Spec_Std=SPEC.std(ddof=1),\n",
    "                F1_Mean=F1.mean(),\n",
    "                F1_Std=F1.std(ddof=1),\n",
    "                mAP_MEAN=MAP.mean(),  # <- add new line\n",
    "                mAP_STD=MAP.std(ddof=1),  # <- add new line\n",
    "                MCC_Mean=MCC.mean(),\n",
    "                MCC_Std=MCC.std(ddof=1),\n",
    "                AUC_Mean=AUCV.mean(),\n",
    "                AUC_Std=AUCV.std(ddof=1),\n",
    "                # NEW timing summaries\n",
    "                TrainTime_MeanSec=TSEC.mean(),\n",
    "                TrainTime_StdSec=TSEC.std(ddof=1),\n",
    "                TrainTime_PerEpoch_MeanSec=TPE.mean(),\n",
    "                TrainTime_PerEpoch_StdSec=TPE.std(ddof=1),\n",
    "            )\n",
    "            rows.append(row)\n",
    "\n",
    "        # === McNemar P-values vs reference model (aggregate across folds)\n",
    "        ref = reference_model\n",
    "        if ref not in preds_by_model:\n",
    "            print(\n",
    "                f\"[WARN] reference_model='{ref}' not in models_list; skipping McNemar for {feat_name}\"\n",
    "            )\n",
    "        else:\n",
    "            for row in rows:\n",
    "                model_name = row[\"Model\"]\n",
    "                if model_name == ref:\n",
    "                    row[\"McNemar_P\"] = np.nan\n",
    "                    row[\"McNemar_n01\"] = np.nan\n",
    "                    row[\"McNemar_n10\"] = np.nan\n",
    "                else:\n",
    "                    p, n01, n10 = _mcnemar_from_folds(\n",
    "                        y_true_folds_any,\n",
    "                        preds_by_model[ref],\n",
    "                        preds_by_model[model_name],\n",
    "                    )\n",
    "                    row[\"McNemar_P\"] = p\n",
    "                    row[\"McNemar_n01\"] = n01\n",
    "                    row[\"McNemar_n10\"] = n10\n",
    "\n",
    "        # append to global results\n",
    "        results.extend(rows)\n",
    "\n",
    "        # === Save per-sample CV outputs in a tidy table ===\n",
    "        rows_long = []\n",
    "        for model_name in models_list:\n",
    "            # each is a list of k arrays\n",
    "            ytf_list = y_true_by_model[model_name]\n",
    "            ypf_list = proba_by_model[model_name]\n",
    "            ypred_list = preds_by_model[model_name]\n",
    "            idx_list = va_idx_by_model[model_name]\n",
    "\n",
    "            for fold_id, (idxs, yt, yp, yhat) in enumerate(\n",
    "                zip(idx_list, ytf_list, ypf_list, ypred_list), start=1\n",
    "            ):\n",
    "                # sanity: all same length\n",
    "                n = len(yt)\n",
    "                assert len(yp) == n and len(yhat) == n and len(idxs) == n\n",
    "                rows_long.append(\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"Feature_Set\": feat_name,\n",
    "                            \"Model\": model_name,\n",
    "                            \"Fold\": fold_id,\n",
    "                            \"orig_idx\": idxs,  # position in original X_all\n",
    "                            \"y_true\": yt.astype(int),\n",
    "                            \"proba\": yp.astype(float),\n",
    "                            \"y_pred\": yhat.astype(int),\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        cv_outputs_df = pd.concat(rows_long, ignore_index=True)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        out_long_path = os.path.join(output_dir, f\"CV_outputs_{feat_name}.csv\")\n",
    "        cv_outputs_df.to_csv(out_long_path, index=False)\n",
    "        print(f\"Saved per-sample CV outputs for {feat_name} -> {out_long_path}\")\n",
    "\n",
    "        if combine_features:\n",
    "            break\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_csv_path = os.path.join(output_dir, \"Table1_CV_all_features.csv\")\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "    print(f\"\\nAll-feature CV Table‑1 saved to {results_csv_path}\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246dfe61",
   "metadata": {
    "papermill": {
     "duration": 0.006417,
     "end_time": "2025-08-25T19:37:20.060824",
     "exception": false,
     "start_time": "2025-08-25T19:37:20.054407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7) Main orchestration (Run Tuned Models for CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495b8a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:20.095669Z",
     "iopub.status.busy": "2025-08-25T19:37:20.095378Z",
     "iopub.status.idle": "2025-08-26T01:30:09.270544Z",
     "shell.execute_reply": "2025-08-26T01:30:09.269667Z"
    },
    "papermill": {
     "duration": 21169.184092,
     "end_time": "2025-08-26T01:30:09.272119",
     "exception": false,
     "start_time": "2025-08-25T19:37:20.088027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2) Run your existing k-fold CV pipeline (now it will pick up best_hparams.json)\n",
    "feature_list   = [\"AAC\",\"APAAC\",\"DPC\",\"ONEHOT\",\"PAAC\",\"PCP\",\"PSSM\",\"PSTPP\",\"F1\",\"F2\"]\n",
    "models_list    = [\"CNN\", \"FCNN\",\"RNN\",\"GRU\",\"LSTM\"] \n",
    "results_df = train_val_pipeline(\n",
    "    feature_list=feature_list,\n",
    "    fasta_list=train_fasta_list,\n",
    "    models_list=models_list,\n",
    "    combine_features=False,\n",
    "    output_dir=\"results_kfold\",  # <-- your desired output dir\n",
    "    # keep your existing CV args the same here\n",
    "    cv_splits=5,  # <-- K-folds\n",
    "    reference_model=\"CNN\",  # <-- choose your reference (e.g., 'CNN' or 'FCNN')\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    cutoff=0.5,\n",
    "    verbose=0,\n",
    ")\n",
    "results_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8095417,
     "sourceId": 12847923,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8139062,
     "sourceId": 12866973,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21216.596885,
   "end_time": "2025-08-26T01:30:13.006194",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-25T19:36:36.409309",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
